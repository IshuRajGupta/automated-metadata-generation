{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0751e69e",
   "metadata": {},
   "source": [
    "\n",
    "# Automated Metadata Generation\n",
    "\n",
    "This notebook demonstrates an end-to-end pipeline for automatically generating metadata from various document types. The process includes text extraction, OCR for scanned documents, and metadata generation (summary and file statistics) using a pre-trained NLP model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae62f64",
   "metadata": {},
   "source": [
    "\n",
    "## Step 1: Install Dependencies\n",
    "\n",
    "First, we need to ensure all the required libraries are installed. You can install them by running the following command in your terminal. The `requirements.txt` file should be in the same directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6ce33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9dbaff",
   "metadata": {},
   "source": [
    "\n",
    "## Step 2: Import Necessary Libraries\n",
    "\n",
    "Here, we import all the libraries needed for file handling, text extraction, OCR, and NLP.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c738d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import docx\n",
    "import PyPDF2\n",
    "import pdfplumber\n",
    "import pytesseract\n",
    "from pdf2image import convert_from_path\n",
    "from PIL import Image\n",
    "from transformers import pipeline\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d3608f",
   "metadata": {},
   "source": [
    "\n",
    "## Step 3: Configure OCR Engine (Tesseract)\n",
    "\n",
    "For the OCR functionality to work, you must have Tesseract-OCR installed on your system and added to your PATH. If it's installed in a custom location, you can uncomment the line below and set the path to your Tesseract executable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9f2113",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# On Windows, you might need to specify the path if it's not in your system's PATH\n",
    "# e.g., pytesseract.pytesseract.tesseract_cmd = r'C:\\\\Program Files\\\\Tesseract-OCR\\\\tesseract.exe'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319c7d8a",
   "metadata": {},
   "source": [
    "\n",
    "## Step 4: Text Extraction Functions\n",
    "\n",
    "These functions handle the extraction of text from different file formats (`.txt`, `.docx`, `.pdf`). For PDFs, it first attempts direct text extraction and then falls back to OCR if needed. **Note:** The OCR function (`extract_text_from_scanned_pdf`) requires Poppler to be installed and in your PATH.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d64a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_text_from_txt(file_path):\n",
    "    \"\"\"Extracts text from a .txt file.\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return f.read()\n",
    "\n",
    "def extract_text_from_docx(file_path):\n",
    "    \"\"\"Extracts text from a .docx file.\"\"\"\n",
    "    doc = docx.Document(file_path)\n",
    "    return '\\n'.join([para.text for para in doc.paragraphs])\n",
    "\n",
    "def extract_text_from_pdf(file_path):\n",
    "    \"\"\"Extracts text from a text-based .pdf file.\"\"\"\n",
    "    text = ''\n",
    "    try:\n",
    "        with pdfplumber.open(file_path) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                page_text = page.extract_text()\n",
    "                if page_text:\n",
    "                    text += page_text + '\\n'\n",
    "    except Exception as e:\n",
    "        print(f\"Error with pdfplumber: {e}, trying PyPDF2\")\n",
    "        try:\n",
    "            with open(file_path, 'rb') as f:\n",
    "                reader = PyPDF2.PdfReader(f)\n",
    "                for page in reader.pages:\n",
    "                    text += page.extract_text() + '\\n'\n",
    "        except Exception as e2:\n",
    "            print(f\"Error with PyPDF2: {e2}\")\n",
    "    return text\n",
    "\n",
    "def extract_text_from_scanned_pdf(file_path):\n",
    "    \"\"\"Extracts text from a scanned (image-based) .pdf file using OCR.\"\"\"\n",
    "    try:\n",
    "        images = convert_from_path(file_path)\n",
    "        text = ''\n",
    "        for img in images:\n",
    "            text += pytesseract.image_to_string(img) + '\\n'\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error during OCR: {e}\")\n",
    "        return ''\n",
    "\n",
    "def extract_text(file_path):\n",
    "    \"\"\"A master function to extract text from any supported file type.\"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        return 'File not found'\n",
    "        \n",
    "    file_extension = os.path.splitext(file_path)[1].lower()\n",
    "    text = ''\n",
    "\n",
    "    if file_extension == '.txt':\n",
    "        text = extract_text_from_txt(file_path)\n",
    "    elif file_extension == '.docx':\n",
    "        text = extract_text_from_docx(file_path)\n",
    "    elif file_extension == '.pdf':\n",
    "        text = extract_text_from_pdf(file_path)\n",
    "        if not text or len(text.strip()) < 100:\n",
    "            print(f'Standard text extraction yielded little or no text for {os.path.basename(file_path)}, trying OCR.')\n",
    "            text += extract_text_from_scanned_pdf(file_path)\n",
    "    else:\n",
    "        print(f'Unsupported file type: {file_extension}')\n",
    "        \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb4549c",
   "metadata": {},
   "source": [
    "\n",
    "## Step 5: Metadata Generation Functions\n",
    "\n",
    "These functions use the extracted text to generate structured metadata. This includes generating a summary using a pre-trained model from Hugging Face and collecting basic file statistics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9755a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SUMMARIZATION_MODEL = \"google/pegasus-xsum\"\n",
    "summarizer = None\n",
    "\n",
    "def initialize_summarizer():\n",
    "    \"\"\"Initializes the summarization pipeline.\"\"\"\n",
    "    global summarizer\n",
    "    if summarizer is None:\n",
    "        print(\"Initializing summarization model...\")\n",
    "        summarizer = pipeline(\"summarization\", model=SUMMARIZATION_MODEL)\n",
    "        print(\"Model initialized.\")\n",
    "\n",
    "def generate_summary(text, max_length=150, min_length=30):\n",
    "    \"\"\"Generates a summary for the given text.\"\"\"\n",
    "    if summarizer is None:\n",
    "        initialize_summarizer()\n",
    "    \n",
    "    max_input_length = 4096\n",
    "    if len(text) > max_input_length:\n",
    "        text = text[:max_input_length]\n",
    "    \n",
    "    if not text.strip():\n",
    "        return \"(No text to summarize)\"\n",
    "\n",
    "    summary_list = summarizer(text, max_length=max_length, min_length=min_length, do_sample=False)\n",
    "    return summary_list[0]['summary_text']\n",
    "\n",
    "def get_file_stats(file_path, text_content):\n",
    "    \"\"\"Generates basic statistics for a file.\"\"\"\n",
    "    return {\n",
    "        \"file_name\": os.path.basename(file_path),\n",
    "        \"file_size_kb\": round(os.path.getsize(file_path) / 1024, 2),\n",
    "        \"word_count\": len(text_content.split())\n",
    "    }\n",
    "\n",
    "def generate_metadata(file_path):\n",
    "    \"\"\"The main function to generate a full set of metadata for a file.\"\"\"\n",
    "    print(f\"Processing file: {file_path}\")\n",
    "    text_content = extract_text(file_path)\n",
    "    if not text_content or text_content == 'File not found':\n",
    "        print(f\"Could not extract text from {file_path}\")\n",
    "        return None\n",
    "\n",
    "    summary = generate_summary(text_content)\n",
    "    stats = get_file_stats(file_path, text_content)\n",
    "\n",
    "    metadata = {\n",
    "        \"summary\": summary,\n",
    "        **stats\n",
    "    }\n",
    "    \n",
    "    return metadata\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4446bba6",
   "metadata": {},
   "source": [
    "\n",
    "## Step 6: Run the Pipeline\n",
    "\n",
    "Now, let's test the entire pipeline. We will specify the path to a document and call the `generate_metadata` function. The first time you run this, it will download the summarization model, which may take a few minutes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63c4d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Make sure the 'documents' folder exists and contains your test file\n",
    "if not os.path.exists('documents'):\n",
    "    os.makedirs('documents')\n",
    "    with open('documents/test.txt', 'w') as f:\n",
    "        f.write('This is a test text file for the notebook. It is short and simple.')\n",
    "\n",
    "test_file_path = 'documents/test.txt' \n",
    "# You can change this to a .docx or .pdf file in the 'documents' folder\n",
    "# test_file_path = 'documents/my_document.pdf'\n",
    "\n",
    "# Generate the metadata\n",
    "metadata_result = generate_metadata(test_file_path)\n",
    "\n",
    "# Print the result in a clean JSON format\n",
    "if metadata_result:\n",
    "    print(\"\\n--- Generated Metadata ---\")\n",
    "    print(json.dumps(metadata_result, indent=2))\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
